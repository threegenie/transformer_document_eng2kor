[ğŸ”— Docs >> Using Transformers >> Summary of the tasks](https://huggingface.co/transformers/task_summary.html)

ì´ í˜ì´ì§€ì—ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì‹œ ê°€ì¥ ë§ì´ ì ìš©ë˜ëŠ” ì‚¬ë¡€ê°€ ì†Œê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë¸ë“¤ì€ ë‹¤ì–‘í•œ êµ¬ì„±ê³¼ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ê°€ì¥ ê°„ë‹¨í•œ ê²ƒì€ ì§ˆë¬¸ ë‹µë³€(question answering), ì‹œí€€ìŠ¤ ë¶„ë¥˜(sequence classification), ê°œì²´ëª… ì¸ì‹(named entity recognition) ë“±ê³¼ ê°™ì€ ì‘ì—…ì— ëŒ€í•œ ì‚¬ë¡€ë“¤ì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì˜ˆì œì—ì„œëŠ” ì˜¤í† ëª¨ë¸(auto-models)ì„ í™œìš©í•©ë‹ˆë‹¤. ì˜¤í† ëª¨ë¸ì€ ì£¼ì–´ì§„ ì²´í¬í¬ì¸íŠ¸ì— ë”°ë¼ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì˜¬ë°”ë¥¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [AutoModel](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModel) ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤. ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ì½”ë“œë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜ì •í•˜ê³ , íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì— ë§ê²Œ ììœ ë¡­ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì´ ì˜ ì‹¤í–‰ë˜ë ¤ë©´ í•´ë‹¹ íƒœìŠ¤í¬ì— í•´ë‹¹í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´í¬í¬ì¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì§‘í•©ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¦¬íŠ¸ë ˆì¸ë˜ê³  íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•´ íŒŒì¸íŠœë‹ ë©ë‹ˆë‹¤. ì´ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- ëª¨ë“  ëª¨ë¸ì´ ëª¨ë“  íƒœìŠ¤í¬ì— ëŒ€í•´ íŒŒì¸íŠœë‹ëœ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. íŠ¹ì • íƒœìŠ¤í¬ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ [ì˜ˆì œ ë””ë ‰í† ë¦¬](https://github.com/huggingface/transformers/tree/master/examples)ì˜ *run_$TASK.py*ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì€ íŠ¹ì • ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¸íŠœë‹ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì‚¬ìš© ì˜ˆì œ ë° ë„ë©”ì¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ [ì˜ˆì œ](https://github.com/huggingface/transformers/tree/master/examples) ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê±°ë‚˜ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¶”ë¡  íƒœìŠ¤í¬ë¥¼ ìœ„í•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ëª‡ ê°€ì§€ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- íŒŒì´í”„ë¼ì¸ : ì‚¬ìš©í•˜ê¸° ë§¤ìš° ì‰¬ìš´ ë°©ì‹ìœ¼ë¡œ, ë‘ ì¤„ì˜ ì½”ë“œë¡œ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ì§ì ‘ ëª¨ë¸ ì‚¬ìš©í•˜ê¸° : ì¶”ìƒí™”ê°€ ëœ ë˜ì§€ë§Œ, í† í¬ë‚˜ì´ì €(íŒŒì´í† ì¹˜/í…ì„œí”Œë¡œìš°)ì— ì§ì ‘ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ìœ ì—°ì„±ê³¼ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.

ì—¬ê¸°ì— ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì´ ëª¨ë‘ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

> ğŸ’› ì£¼ì˜
ì—¬ê¸°ì— ì œì‹œëœ ëª¨ë“  íƒœìŠ¤í¬ì—ì„œëŠ” íŠ¹ì • íƒœìŠ¤í¬ì— ë§ê²Œ íŒŒì¸íŠœë‹ëœ í”„ë¦¬íŠ¸ë ˆì¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ í™œìš©í•©ë‹ˆë‹¤. íŠ¹ì • ì‘ì—…ì—ì„œ íŒŒì¸íŠœë‹ ë˜ì§€ ì•Šì€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ë©´ íƒœìŠ¤í¬ì— ì‚¬ìš©ë˜ëŠ” ì¶”ê°€ í—¤ë“œê°€ ì•„ë‹Œ ê¸°ë³¸ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë§Œ ë¡œë“œë˜ì–´ í•´ë‹¹ í—¤ë“œì˜ ê°€ì¤‘ì¹˜ê°€ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.
ì´ë ‡ê²Œ í•˜ë©´ ëœë¤ìœ¼ë¡œ ì¶œë ¥ì´ ìƒì„±ë©ë‹ˆë‹¤.
>

### ì‹œí€€ìŠ¤ ë¶„ë¥˜(Sequence Classification)

ì‹œí€€ìŠ¤ ë¶„ë¥˜ëŠ” ì£¼ì–´ì§„ í´ë˜ìŠ¤ ìˆ˜ì— ë”°ë¼ ì‹œí€€ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” íƒœìŠ¤í¬ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ë¶„ë¥˜ì˜ ì˜ˆì‹œë¡œëŠ” ì´ íƒœìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” GLUE ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤. GLUE ì‹œí€€ìŠ¤ ë¶„ë¥˜ íƒœìŠ¤í¬ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹ í•˜ë ¤ë©´ [*run_glue.py*](https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py), *[run_tf_glue.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_tf_glue.py)*, *[run_tf_classification.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_tf_text_classification.py)* ë˜ëŠ” *[run_xnli.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_xnli.py)* ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ê°€ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ë¥¼ ì‹ë³„í•˜ì—¬ ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤. GLUE íƒœìŠ¤í¬ì¸ sst2ì—ì„œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í™œìš©í•©ë‹ˆë‹¤.

ì´ë ‡ê²Œ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìŠ¤ì½”ì–´ì™€ í•¨ê»˜ ë¼ë²¨(POSITIVE-ê¸ì • or NEGATIVE-ë¶€ì •)ì´ ë°˜í™˜ë©ë‹ˆë‹¤.

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

result = classifier("I hate you")[0]
print(f"label: {result['label']}, with score: {round(result['score'], 4)}")

result = classifier("I love you")[0]
print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
```

ë‹¤ìŒì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ì‹œí€€ìŠ¤ê°€ ì„œë¡œ ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ë¥¸ ë¬¸ì¥ì¸ì§€ì˜ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ì‹œí€€ìŠ¤ ë¶„ë¥˜ì˜ ì˜ˆì…ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ì—ì„œ í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì€ BERT ëª¨ë¸ë¡œì„œ ì‹ë³„ë˜ë©° ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥ëœ ê°€ì¤‘ì¹˜ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
2. ì˜¬ë°”ë¥¸ ëª¨ë¸ë³„ êµ¬ë¶„ ê¸°í˜¸, í† í° ìœ í˜• ID ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬(í† í¬ë‚˜ì´ì €ì— ì˜í•´ ìë™ìœ¼ë¡œ ì‘ì„±ë¨)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ë¬¸ì¥ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
3. ëª¨ë¸ì„ í†µí•´ ì´ ì‹œí€€ìŠ¤ë¥¼ ì „ë‹¬í•˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œ ë‘ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ì¸ 0(íŒŒë¼í”„ë ˆì´ìŠ¤ê°€ ì•„ë‹˜)ê³¼ 1(íŒŒë¼í”„ë ˆì´ìŠ¤ì„) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
4. í´ë˜ìŠ¤ ë¶„ë¥˜ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ê²°ê³¼ì— ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.
5. ê²°ê³¼ë¥¼ í”„ë¦°íŠ¸í•©ë‹ˆë‹¤.

```python
# Pytorch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

classes = ["not paraphrase", "is paraphrase"]

sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to
# the sequence, as well as compute the attention masks.
paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="pt")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="pt")

paraphrase_classification_logits = model(**paraphrase).logits
not_paraphrase_classification_logits = model(**not_paraphrase).logits

paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]
not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]

# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")
"""
not paraphrase: 10%
is paraphrase: 90%
"""

# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
"""
not paraphrase: 94%
is paraphrase: 6%
"""
```

```python
# Tensorflow
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

classes = ["not paraphrase", "is paraphrase"]

sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to
# the sequence, as well as compute the attention masks.
paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="tf")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="tf")

paraphrase_classification_logits = model(paraphrase).logits
not_paraphrase_classification_logits = model(not_paraphrase).logits

paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]
not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]

# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")
"""
not paraphrase: 10%
is paraphrase: 90%
"""

# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
"""
not paraphrase: 94%
is paraphrase: 6%
"""
```

### ì¶”ì¶œ ì§ˆì˜ì‘ë‹µ(Extractive Question Answering)

ì¶”ì¶œ ì§ˆì˜ì‘ë‹µì€ ì£¼ì–´ì§„ ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì¶”ì¶œí•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì§ˆë¬¸ ë‹µë³€ ë°ì´í„°ì…‹ì˜ ì˜ˆë¡œëŠ” í•´ë‹¹ ì‘ì—…ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” SQuAD ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤. SQuAD ì‘ì—…ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ *[run_qa.py](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering/run_qa.py)* ë° *[run_tf_squad.py](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/question-answering/run_tf_squad.py)* ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ì„ ì¶”ì¶œí•˜ëŠ” ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤. SQuAD ë°ì´í„°ì…‹ì„ í†µí•´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í™œìš©í•©ë‹ˆë‹¤.

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a
question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune
a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.
"""
```

ì´ë ‡ê²Œ í•˜ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ **ë‹µë³€**ê³¼ **ì‹ ë¢° ì ìˆ˜(confidence score)**ê°€ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ë‹µë³€ì˜ **ìœ„ì¹˜**ì¸ 'ì‹œì‘' ë° 'ì¢…ë£Œ' ê°’ê³¼ í•¨ê»˜ ë°˜í™˜ë©ë‹ˆë‹¤.

```python
result = question_answerer(question="What is extractive question answering?", context=context)
print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}")

result = question_answerer(question="What is a good example of a question answering dataset?", context=context)
print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}")
```

ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ì—ì„œ í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì€ BERT ëª¨ë¸ë¡œ ì‹ë³„ë˜ë©° ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥ëœ ê°€ì¤‘ì¹˜ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
2. í…ìŠ¤íŠ¸ì™€ ëª‡ ê°€ì§€ ì§ˆë¬¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
3. ì§ˆë¬¸ì„ ë°˜ë³µí•˜ê³  ì˜¬ë°”ë¥¸ ëª¨ë¸ë³„ ì‹ë³„ì í† í° íƒ€ì… ID ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ í˜„ì¬ ì§ˆë¬¸ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
4. ì´ ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì‹œì‘ ìœ„ì¹˜ì™€ ë ìœ„ì¹˜ ëª¨ë‘ì— ëŒ€í•´ ì „ì²´ ì‹œí€€ìŠ¤ í† í°(ì§ˆë¬¸ê³¼ í…ìŠ¤íŠ¸)ì— ê±¸ì³ ë‹¤ì–‘í•œ ì ìˆ˜ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.
5. í† í°ì— ëŒ€í•œ í™•ë¥ ì„ ì–»ê¸° ìœ„í•´ ê²°ê³¼ê°’ì— ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì·¨í•©ë‹ˆë‹¤.
6. ì‹ë³„ëœ ì‹œì‘ ë° ë ìœ„ì¹˜ì—ì„œ í† í°ì„ ê°€ì ¸ì™€ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
7. ê²°ê³¼ë¥¼ í”„ë¦°íŠ¸í•©ë‹ˆë‹¤.

```python
# Pytorch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = AutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

text = r"""
ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
TensorFlow 2.0 and PyTorch.
"""

questions = [
    "How many pretrained models are available in ğŸ¤— Transformers?",
    "What does ğŸ¤— Transformers provide?",
    "ğŸ¤— Transformers provides interoperability between which frameworks?",
]

for question in questions:
    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]
    outputs = model(**inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits
    # Get the most likely beginning of answer with the argmax of the score
    answer_start = torch.argmax(answer_start_scores)
    # Get the most likely end of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    print(f"Question: {question}")
    print(f"Answer: {answer}")

"""
Question: How many pretrained models are available in ğŸ¤— Transformers?
Answer: over 32 +
Question: What does ğŸ¤— Transformers provide?
Answer: general - purpose architectures
Question: ğŸ¤— Transformers provides interoperability between which frameworks?
Answer: tensorflow 2. 0 and pytorch
"""
```

```python
# Tensorflow
from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

text = r"""
ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
TensorFlow 2.0 and PyTorch.
"""

questions = [
    "How many pretrained models are available in ğŸ¤— Transformers?",
    "What does ğŸ¤— Transformers provide?",
    "ğŸ¤— Transformers provides interoperability between which frameworks?",
]

for question in questions:
    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors="tf")
    input_ids = inputs["input_ids"].numpy()[0]
    outputs = model(inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits
    # Get the most likely beginning of answer with the argmax of the score
    answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]
    # Get the most likely end of answer with the argmax of the score
    answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    print(f"Question: {question}")
    print(f"Answer: {answer}")

"""
Question: How many pretrained models are available in ğŸ¤— Transformers?
Answer: over 32 +
Question: What does ğŸ¤— Transformers provide?
Answer: general - purpose architectures
Question: ğŸ¤— Transformers provides interoperability between which frameworks?
Answer: tensorflow 2. 0 and pytorch
"""
```

### ì–¸ì–´ ëª¨ë¸ë§(Language Modeling)

ì–¸ì–´ ëª¨ë¸ë§ì€ ëª¨ë¸ì„ ì½”í¼ìŠ¤ì— ë§ì¶”ëŠ” ì‘ì—…ì´ë©°, íŠ¹ì • ë„ë©”ì¸ì— íŠ¹í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì€ ì–¸ì–´ ëª¨ë¸ë§ì„ ë³€í˜•(ì˜ˆ: ë§ˆìŠ¤í¬ëœ ì–¸ì–´ ëª¨ë¸ë§ì„ ì‚¬ìš©í•œ BERT, ì¼ìƒ ì–¸ì–´ ëª¨ë¸ë§ì„ ì‚¬ìš©í•œ GPT-2)í•˜ì—¬ í›ˆë ¨ë©ë‹ˆë‹¤.

ì–¸ì–´ ëª¨ë¸ë§ì€ í”„ë¦¬íŠ¸ë ˆì´ë‹ ì´ì™¸ì—ë„ ëª¨ë¸ ë°°í¬ë¥¼ ê° ë„ë©”ì¸ì— ë§ê²Œ íŠ¹í™”ì‹œí‚¤ê¸° ìœ„í•´ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëŒ€ìš©ëŸ‰ ì½”í¼ìŠ¤ë¥¼ í†µí•´ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë‹¤ìŒ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë˜ëŠ” ê³¼í•™ ë…¼ë¬¸ ë°ì´í„°ì…‹(ì˜ˆ : [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp))ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ë§ˆìŠ¤í¬ëœ ì–¸ì–´ ëª¨ë¸ë§(Masked Language Modeling)**

ë§ˆìŠ¤í¬ëœ ì–¸ì–´ ëª¨ë¸ë§ì€ ë§ˆìŠ¤í‚¹ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ìˆœì„œëŒ€ë¡œ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê³  ëª¨ë¸ì´ í•´ë‹¹ ë§ˆìŠ¤í¬ë¥¼ ì ì ˆí•œ í† í°ìœ¼ë¡œ ì±„ìš°ë„ë¡ ìš”ì²­í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì´ ì˜¤ë¥¸ìª½ ì»¨í…ìŠ¤íŠ¸(ë§ˆìŠ¤í¬ ì˜¤ë¥¸ìª½ì˜ í† í°)ì™€ ì™¼ìª½ ì»¨í…ìŠ¤íŠ¸(ë§ˆìŠ¤í¬ ì™¼ìª½ì˜ í† í°)ë¥¼ ëª¨ë‘ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í›ˆë ¨ì€ SQuAD(ì§ˆì˜ì‘ë‹µ, [Lewis, Lui, Goyal et al](https://arxiv.org/abs/1910.13461), íŒŒíŠ¸ 4.2)ì™€ ê°™ì€ ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ê°•ë ¥í•œ ê¸°ì´ˆ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤. ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—…ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ *[run_mlm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py)* ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ì—ì„œ ë§ˆìŠ¤í¬ë¥¼ êµì²´í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤.

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
```
