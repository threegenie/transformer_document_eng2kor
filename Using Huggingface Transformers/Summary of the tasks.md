[ğŸ”— Docs >> Using Transformers >> Summary of the tasks](https://huggingface.co/transformers/task_summary.html)

ì´ í˜ì´ì§€ì—ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì‹œ ê°€ì¥ ë§ì´ ì ìš©ë˜ëŠ” ì‚¬ë¡€ê°€ ì†Œê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë¸ë“¤ì€ ë‹¤ì–‘í•œ êµ¬ì„±ê³¼ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ê°€ì¥ ê°„ë‹¨í•œ ê²ƒì€ ì§ˆë¬¸ ë‹µë³€(question answering), ì‹œí€€ìŠ¤ ë¶„ë¥˜(sequence classification), ê°œì²´ëª… ì¸ì‹(named entity recognition) ë“±ê³¼ ê°™ì€ ì‘ì—…ì— ëŒ€í•œ ì‚¬ë¡€ë“¤ì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì˜ˆì œì—ì„œëŠ” ì˜¤í† ëª¨ë¸(auto-models)ì„ í™œìš©í•©ë‹ˆë‹¤. ì˜¤í† ëª¨ë¸ì€ ì£¼ì–´ì§„ ì²´í¬í¬ì¸íŠ¸ì— ë”°ë¼ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì˜¬ë°”ë¥¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [AutoModel](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModel) ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤. ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ì½”ë“œë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜ì •í•˜ê³ , íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì— ë§ê²Œ ììœ ë¡­ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì´ ì˜ ì‹¤í–‰ë˜ë ¤ë©´ í•´ë‹¹ íƒœìŠ¤í¬ì— í•´ë‹¹í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´í¬í¬ì¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì§‘í•©ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¦¬íŠ¸ë ˆì¸ë˜ê³  íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•´ íŒŒì¸íŠœë‹ ë©ë‹ˆë‹¤. ì´ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- ëª¨ë“  ëª¨ë¸ì´ ëª¨ë“  íƒœìŠ¤í¬ì— ëŒ€í•´ íŒŒì¸íŠœë‹ëœ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. íŠ¹ì • íƒœìŠ¤í¬ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ [ì˜ˆì œ ë””ë ‰í† ë¦¬](https://github.com/huggingface/transformers/tree/master/examples)ì˜ *run_$TASK.py*ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì€ íŠ¹ì • ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¸íŠœë‹ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì‚¬ìš© ì˜ˆì œ ë° ë„ë©”ì¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ [ì˜ˆì œ](https://github.com/huggingface/transformers/tree/master/examples) ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê±°ë‚˜ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¶”ë¡  íƒœìŠ¤í¬ë¥¼ ìœ„í•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ëª‡ ê°€ì§€ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- íŒŒì´í”„ë¼ì¸ : ì‚¬ìš©í•˜ê¸° ë§¤ìš° ì‰¬ìš´ ë°©ì‹ìœ¼ë¡œ, ë‘ ì¤„ì˜ ì½”ë“œë¡œ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ì§ì ‘ ëª¨ë¸ ì‚¬ìš©í•˜ê¸° : ì¶”ìƒí™”ê°€ ëœ ë˜ì§€ë§Œ, í† í¬ë‚˜ì´ì €(íŒŒì´í† ì¹˜/í…ì„œí”Œë¡œìš°)ì— ì§ì ‘ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ìœ ì—°ì„±ê³¼ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.

ì—¬ê¸°ì— ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì´ ëª¨ë‘ ì œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

> ğŸ’› ì£¼ì˜
> 
> ì—¬ê¸°ì— ì œì‹œëœ ëª¨ë“  íƒœìŠ¤í¬ì—ì„œëŠ” íŠ¹ì • íƒœìŠ¤í¬ì— ë§ê²Œ íŒŒì¸íŠœë‹ëœ í”„ë¦¬íŠ¸ë ˆì¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ í™œìš©í•©ë‹ˆë‹¤. íŠ¹ì • ì‘ì—…ì—ì„œ íŒŒì¸íŠœë‹ ë˜ì§€ ì•Šì€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ë©´ íƒœìŠ¤í¬ì— ì‚¬ìš©ë˜ëŠ” ì¶”ê°€ í—¤ë“œê°€ ì•„ë‹Œ ê¸°ë³¸ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë§Œ ë¡œë“œë˜ì–´ í•´ë‹¹ í—¤ë“œì˜ ê°€ì¤‘ì¹˜ê°€ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.
ì´ë ‡ê²Œ í•˜ë©´ ëœë¤ìœ¼ë¡œ ì¶œë ¥ì´ ìƒì„±ë©ë‹ˆë‹¤.
>

### ì‹œí€€ìŠ¤ ë¶„ë¥˜(Sequence Classification)

ì‹œí€€ìŠ¤ ë¶„ë¥˜ëŠ” ì£¼ì–´ì§„ í´ë˜ìŠ¤ ìˆ˜ì— ë”°ë¼ ì‹œí€€ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” íƒœìŠ¤í¬ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ë¶„ë¥˜ì˜ ì˜ˆì‹œë¡œëŠ” ì´ íƒœìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” GLUE ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤. GLUE ì‹œí€€ìŠ¤ ë¶„ë¥˜ íƒœìŠ¤í¬ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹ í•˜ë ¤ë©´ [*run_glue.py*](https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py), *[run_tf_glue.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_tf_glue.py)*, *[run_tf_classification.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/text-classification/run_tf_text_classification.py)* ë˜ëŠ” *[run_xnli.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_xnli.py)* ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ê°€ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ë¥¼ ì‹ë³„í•˜ì—¬ ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤. GLUE íƒœìŠ¤í¬ì¸ sst2ì—ì„œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í™œìš©í•©ë‹ˆë‹¤.

ì´ë ‡ê²Œ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìŠ¤ì½”ì–´ì™€ í•¨ê»˜ ë¼ë²¨(POSITIVE-ê¸ì • or NEGATIVE-ë¶€ì •)ì´ ë°˜í™˜ë©ë‹ˆë‹¤.

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

result = classifier("I hate you")[0]
print(f"label: {result['label']}, with score: {round(result['score'], 4)}")

result = classifier("I love you")[0]
print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
```

ë‹¤ìŒì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ì‹œí€€ìŠ¤ê°€ ì„œë¡œ ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ë¥¸ ë¬¸ì¥ì¸ì§€ì˜ ì—¬ë¶€(paraphrase or not)ë¥¼ ê²°ì •í•˜ëŠ” ì‹œí€€ìŠ¤ ë¶„ë¥˜ì˜ ì˜ˆì…ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ì—ì„œ í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì€ BERT ëª¨ë¸ë¡œì„œ ì‹ë³„ë˜ë©° ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥ëœ ê°€ì¤‘ì¹˜ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
2. ì˜¬ë°”ë¥¸ ëª¨ë¸ë³„ êµ¬ë¶„ ê¸°í˜¸, í† í° ìœ í˜• ID ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬(í† í¬ë‚˜ì´ì €ì— ì˜í•´ ìë™ìœ¼ë¡œ ì‘ì„±ë¨)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ë¬¸ì¥ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
3. ëª¨ë¸ì„ í†µí•´ ì´ ì‹œí€€ìŠ¤ë¥¼ ì „ë‹¬í•˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œ ë‘ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ì¸ 0(no paraphrase)ê³¼ 1(paraphrase) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
4. í´ë˜ìŠ¤ ë¶„ë¥˜ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ê²°ê³¼ì— ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.
5. ê²°ê³¼ë¥¼ í”„ë¦°íŠ¸í•©ë‹ˆë‹¤.

```python
# Pytorch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

classes = ["not paraphrase", "is paraphrase"]

sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to
# the sequence, as well as compute the attention masks.
paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="pt")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="pt")

paraphrase_classification_logits = model(**paraphrase).logits
not_paraphrase_classification_logits = model(**not_paraphrase).logits

paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]
not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]

# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")
"""
not paraphrase: 10%
is paraphrase: 90%
"""

# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
"""
not paraphrase: 94%
is paraphrase: 6%
"""
```

```python
# Tensorflow
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

classes = ["not paraphrase", "is paraphrase"]

sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to
# the sequence, as well as compute the attention masks.
paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="tf")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="tf")

paraphrase_classification_logits = model(paraphrase).logits
not_paraphrase_classification_logits = model(not_paraphrase).logits

paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]
not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]

# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")
"""
not paraphrase: 10%
is paraphrase: 90%
"""

# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
"""
not paraphrase: 94%
is paraphrase: 6%
"""
```

### ì¶”ì¶œ ì§ˆì˜ì‘ë‹µ(Extractive Question Answering)

ì¶”ì¶œ ì§ˆì˜ì‘ë‹µì€ ì£¼ì–´ì§„ ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì¶”ì¶œí•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì§ˆë¬¸ ë‹µë³€ ë°ì´í„°ì…‹ì˜ ì˜ˆë¡œëŠ” í•´ë‹¹ ì‘ì—…ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” SQuAD ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤. SQuAD ì‘ì—…ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ *[run_qa.py](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering/run_qa.py)* ë° *[run_tf_squad.py](https://github.com/huggingface/transformers/tree/master/examples/tensorflow/question-answering/run_tf_squad.py)* ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ì„ ì¶”ì¶œí•˜ëŠ” ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤. SQuAD ë°ì´í„°ì…‹ì„ í†µí•´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í™œìš©í•©ë‹ˆë‹¤.

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a
question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune
a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.
"""
```

ì´ë ‡ê²Œ í•˜ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ **ë‹µë³€**ê³¼ **ì‹ ë¢° ì ìˆ˜(confidence score)**ê°€ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ë‹µë³€ì˜ **ìœ„ì¹˜**ì¸ 'ì‹œì‘' ë° 'ì¢…ë£Œ' ê°’ê³¼ í•¨ê»˜ ë°˜í™˜ë©ë‹ˆë‹¤.

```python
result = question_answerer(question="What is extractive question answering?", context=context)
print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}")

result = question_answerer(question="What is a good example of a question answering dataset?", context=context)
print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}")
```

ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ì—ì„œ í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì€ BERT ëª¨ë¸ë¡œ ì‹ë³„ë˜ë©° ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥ëœ ê°€ì¤‘ì¹˜ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
2. í…ìŠ¤íŠ¸ì™€ ëª‡ ê°€ì§€ ì§ˆë¬¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
3. ì§ˆë¬¸ì„ ë°˜ë³µí•˜ê³  ì˜¬ë°”ë¥¸ ëª¨ë¸ë³„ ì‹ë³„ì í† í° íƒ€ì… ID ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ í˜„ì¬ ì§ˆë¬¸ì˜ ì‹œí€€ìŠ¤ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
4. ì´ ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì‹œì‘ ìœ„ì¹˜ì™€ ë ìœ„ì¹˜ ëª¨ë‘ì— ëŒ€í•´ ì „ì²´ ì‹œí€€ìŠ¤ í† í°(ì§ˆë¬¸ê³¼ í…ìŠ¤íŠ¸)ì— ê±¸ì³ ë‹¤ì–‘í•œ ì ìˆ˜ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.
5. í† í°ì— ëŒ€í•œ í™•ë¥ ì„ ì–»ê¸° ìœ„í•´ ê²°ê³¼ê°’ì— ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì·¨í•©ë‹ˆë‹¤.
6. ì‹ë³„ëœ ì‹œì‘ ë° ë ìœ„ì¹˜ì—ì„œ í† í°ì„ ê°€ì ¸ì™€ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
7. ê²°ê³¼ë¥¼ í”„ë¦°íŠ¸í•©ë‹ˆë‹¤.

```python
# Pytorch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = AutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

text = r"""
ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
TensorFlow 2.0 and PyTorch.
"""

questions = [
    "How many pretrained models are available in ğŸ¤— Transformers?",
    "What does ğŸ¤— Transformers provide?",
    "ğŸ¤— Transformers provides interoperability between which frameworks?",
]

for question in questions:
    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]
    outputs = model(**inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits
    # Get the most likely beginning of answer with the argmax of the score
    answer_start = torch.argmax(answer_start_scores)
    # Get the most likely end of answer with the argmax of the score
    answer_end = torch.argmax(answer_end_scores) + 1
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    print(f"Question: {question}")
    print(f"Answer: {answer}")

"""
Question: How many pretrained models are available in ğŸ¤— Transformers?
Answer: over 32 +
Question: What does ğŸ¤— Transformers provide?
Answer: general - purpose architectures
Question: ğŸ¤— Transformers provides interoperability between which frameworks?
Answer: tensorflow 2. 0 and pytorch
"""
```

```python
# Tensorflow
from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

text = r"""
ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural
Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
TensorFlow 2.0 and PyTorch.
"""

questions = [
    "How many pretrained models are available in ğŸ¤— Transformers?",
    "What does ğŸ¤— Transformers provide?",
    "ğŸ¤— Transformers provides interoperability between which frameworks?",
]

for question in questions:
    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors="tf")
    input_ids = inputs["input_ids"].numpy()[0]
    outputs = model(inputs)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits
    # Get the most likely beginning of answer with the argmax of the score
    answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]
    # Get the most likely end of answer with the argmax of the score
    answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    print(f"Question: {question}")
    print(f"Answer: {answer}")

"""
Question: How many pretrained models are available in ğŸ¤— Transformers?
Answer: over 32 +
Question: What does ğŸ¤— Transformers provide?
Answer: general - purpose architectures
Question: ğŸ¤— Transformers provides interoperability between which frameworks?
Answer: tensorflow 2. 0 and pytorch
"""
```

### ì–¸ì–´ ëª¨ë¸ë§(Language Modeling)

ì–¸ì–´ ëª¨ë¸ë§ì€ ëª¨ë¸ì„ ì½”í¼ìŠ¤ì— ë§ì¶”ëŠ” ì‘ì—…ì´ë©°, íŠ¹ì • ë„ë©”ì¸ì— íŠ¹í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì€ ì–¸ì–´ ëª¨ë¸ë§ì„ ë³€í˜•(ì˜ˆ: ë§ˆìŠ¤í¬ëœ ì–¸ì–´ ëª¨ë¸ë§ì„ ì‚¬ìš©í•œ BERT, ì¼ìƒ ì–¸ì–´ ëª¨ë¸ë§ì„ ì‚¬ìš©í•œ GPT-2)í•˜ì—¬ í›ˆë ¨ë©ë‹ˆë‹¤.

ì–¸ì–´ ëª¨ë¸ë§ì€ í”„ë¦¬íŠ¸ë ˆì´ë‹ ì´ì™¸ì—ë„ ëª¨ë¸ ë°°í¬ë¥¼ ê° ë„ë©”ì¸ì— ë§ê²Œ íŠ¹í™”ì‹œí‚¤ê¸° ìœ„í•´ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëŒ€ìš©ëŸ‰ ì½”í¼ìŠ¤ë¥¼ í†µí•´ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë‹¤ìŒ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ë˜ëŠ” ê³¼í•™ ë…¼ë¬¸ ë°ì´í„°ì…‹(ì˜ˆ : [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp))ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ë§ˆìŠ¤í¬ëœ ì–¸ì–´ ëª¨ë¸ë§(Masked Language Modeling)**

ë§ˆìŠ¤í¬ëœ ì–¸ì–´ ëª¨ë¸ë§ì€ ë§ˆìŠ¤í‚¹ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ìˆœì„œëŒ€ë¡œ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê³  ëª¨ë¸ì´ í•´ë‹¹ ë§ˆìŠ¤í¬ë¥¼ ì ì ˆí•œ í† í°ìœ¼ë¡œ ì±„ìš°ë„ë¡ ìš”ì²­í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì´ ì˜¤ë¥¸ìª½ ì»¨í…ìŠ¤íŠ¸(ë§ˆìŠ¤í¬ ì˜¤ë¥¸ìª½ì˜ í† í°)ì™€ ì™¼ìª½ ì»¨í…ìŠ¤íŠ¸(ë§ˆìŠ¤í¬ ì™¼ìª½ì˜ í† í°)ë¥¼ ëª¨ë‘ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í›ˆë ¨ì€ SQuAD(ì§ˆì˜ì‘ë‹µ, [Lewis, Lui, Goyal et al](https://arxiv.org/abs/1910.13461), íŒŒíŠ¸ 4.2)ì™€ ê°™ì€ ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ê°•ë ¥í•œ ê¸°ì´ˆ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤. ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—…ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ *[run_mlm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py)* ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ì—ì„œ ë§ˆìŠ¤í¬ë¥¼ êµì²´í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤.

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
```
ê·¸ëŸ¬ë©´ ë§ˆìŠ¤í¬ê°€ ì±„ì›Œì§„ ì‹œí€€ìŠ¤, ìŠ¤ì½”ì–´ ë° í† í°IDê°€ í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ì¶œë ¥ë©ë‹ˆë‹¤.

```python
from pprint import pprint
pprint(unmasker(f"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks."))
[{'score': 0.1793,
  'sequence': 'HuggingFace is creating a tool that the community uses to solve '
              'NLP tasks.',
  'token': 3944,
  'token_str': ' tool'},
 {'score': 0.1135,
  'sequence': 'HuggingFace is creating a framework that the community uses to '
              'solve NLP tasks.',
  'token': 7208,
  'token_str': ' framework'},
 {'score': 0.0524,
  'sequence': 'HuggingFace is creating a library that the community uses to '
              'solve NLP tasks.',
  'token': 5560,
  'token_str': ' library'},
 {'score': 0.0349,
  'sequence': 'HuggingFace is creating a database that the community uses to '
              'solve NLP tasks.',
  'token': 8503,
  'token_str': ' database'},
 {'score': 0.0286,
  'sequence': 'HuggingFace is creating a prototype that the community uses to '
              'solve NLP tasks.',
  'token': 17715,
  'token_str': ' prototype'}]
```
ë‹¤ìŒì€ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ì—ì„œ í† í¬ë¼ì´ì € ë° ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” DistilBERT ëª¨ë¸ì„ ì‚¬ìš©í•  ê²ƒì´ê³ , ê°€ì¤‘ì¹˜ê°€ ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥ë©ë‹ˆë‹¤.
2. ë‹¨ì–´ ëŒ€ì‹  tokenizer.mask_tokenì„ ë°°ì¹˜í•˜ì—¬ ë§ˆìŠ¤í‚¹ëœ í† í°ìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
3. í•´ë‹¹ ì‹œí€€ìŠ¤ë¥¼ ID ëª©ë¡ìœ¼ë¡œ ì¸ì½”ë”©í•˜ê³  í•´ë‹¹ ëª©ë¡ì—ì„œ ë§ˆìŠ¤í‚¹ëœ í† í°ì˜ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
4. ë§ˆìŠ¤í‚¹ëœ í† í°ì˜ ì¸ë±ìŠ¤ì—ì„œ ì˜ˆì¸¡ê°’ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì´ í…ì„œëŠ” ì–´íœ˜ì™€ í¬ê¸°ê°€ ê°™ê³ , ê°’ì€ ê° í† í°ì— ê·€ì†ë˜ëŠ” ì ìˆ˜ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê·¸ëŸ° ë§¥ë½ì—ì„œ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  ìƒê°ë˜ëŠ” í† í°ì— ë” ë†’ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
5. PyTorch topk ë˜ëŠ” TensorFlow top_k ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ 5ê°œì˜ í† í°ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
6. ë§ˆìŠ¤í‚¹ëœ í† í°ì„ í† í°ìœ¼ë¡œ ë°”ê¾¸ê³  ê²°ê³¼ë¥¼ í”„ë¦°íŠ¸í•©ë‹ˆë‹¤.

```python
# Pytorch
from transformers import AutoModelForMaskedLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-cased")
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-cased")

sequence = "Distilled models are smaller than the models they mimic. Using them instead of the large " \
    f"versions would help {tokenizer.mask_token} our carbon footprint."

inputs = tokenizer(sequence, return_tensors="pt")
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]

token_logits = model(**inputs).logits
mask_token_logits = token_logits[0, mask_token_index, :]

top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))
"""
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.
"""
```

```python
# Tensorflow
from transformers import TFAutoModelForMaskedLM, AutoTokenizer
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-cased")
model = TFAutoModelForMaskedLM.from_pretrained("distilbert-base-cased")

sequence = "Distilled models are smaller than the models they mimic. Using them instead of the large " \
    f"versions would help {tokenizer.mask_token} our carbon footprint."

inputs = tokenizer(sequence, return_tensors="tf")
mask_token_index = tf.where(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]

token_logits = model(**inputs).logits
mask_token_logits = token_logits[0, mask_token_index, :]

top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()

for token in top_5_tokens:
    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))
"""
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.
Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.
"""
```

ëª¨ë¸ì—ì„œ ì˜ˆì¸¡í•œ ìƒìœ„ 5ê°œì˜ í† í°ë“¤ìœ¼ë¡œ ì´ë£¨ì–´ì§„ 5ê°œì˜ ì‹œí€€ìŠ¤ê°€ í”„ë¦°íŠ¸ë©ë‹ˆë‹¤.

### ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§(Causal Language Modeling)

ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§ì€ í† í° ìˆœì„œì— ë”°ë¼ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œëŠ” ëª¨ë¸ì´ ì™¼ìª½ ì»¨í…ìŠ¤íŠ¸(ë§ˆìŠ¤í¬ ì™¼ìª½ì— ìˆëŠ” í† í°)ì—ë§Œ ì§‘ì¤‘í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•™ìŠµ ê³¼ì •ì€ ë¬¸ì¥ ìƒì„± ì‘ì—…ê³¼ íŠ¹íˆ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤. ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—…ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ *run_clm.py* ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ í† í°ì€ ëª¨ë¸ì´ ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ ìƒì„±í•˜ëŠ” ë§ˆì§€ë§‰ íˆë“  ë ˆì´ì–´ì˜ *logit*ì—ì„œ ìƒ˜í”Œë§ë˜ì–´ ì˜ˆì¸¡ë©ë‹ˆë‹¤.

ë‹¤ìŒì€ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  *top_k_top_p_filtering()* ë©”ì†Œë“œë¥¼ í™œìš©í•˜ì—¬ ì¸í’‹ í† í° ì‹œí€€ìŠ¤ì— ë”°ë¼ ë‹¤ìŒ í† í°ì„ ìƒ˜í”Œë§í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤.

```python
# Pytorch

from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering
import torch
from torch import nn

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

sequence = f"Hugging Face is based in DUMBO, New York City, and"

inputs = tokenizer(sequence, return_tensors="pt")
input_ids = inputs["input_ids"]

# get logits of last hidden state
next_token_logits = model(**inputs).logits[:, -1, :]

# filter
filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)

# sample
probs = nn.functional.softmax(filtered_next_token_logits, dim=-1)
next_token = torch.multinomial(probs, num_samples=1)

generated = torch.cat([input_ids, next_token], dim=-1)

resulting_string = tokenizer.decode(generated.tolist()[0])
print(resulting_string)
"""
Hugging Face is based in DUMBO, New York City, and ...
"""
```

```python
# Tensorflow

from transformers import TFAutoModelForCausalLM, AutoTokenizer, tf_top_k_top_p_filtering
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = TFAutoModelForCausalLM.from_pretrained("gpt2")

sequence = f"Hugging Face is based in DUMBO, New York City, and"

inputs = tokenizer(sequence, return_tensors="tf")
input_ids = inputs["input_ids"]

# get logits of last hidden state
next_token_logits = model(**inputs).logits[:, -1, :]

# filter
filtered_next_token_logits = tf_top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)

# sample
next_token = tf.random.categorical(filtered_next_token_logits, dtype=tf.int32, num_samples=1)

generated = tf.concat([input_ids, next_token], axis=1)

resulting_string = tokenizer.decode(generated.numpy().tolist()[0])
print(resulting_string)

"""
Hugging Face is based in DUMBO, New York City, and ...
"""
```

ì´ë ‡ê²Œ í•˜ë©´ ì›ë˜ì˜ ìˆœì„œì— ë”°ë¼ ì¼ê´€ì„± ìˆëŠ” ë‹¤ìŒ í† í°ì´ ì¶œë ¥ë©ë‹ˆë‹¤. ì´ í† í°ì€ ìš°ë¦¬ì˜ ê²½ìš° ë‹¨ì–´ ë˜ëŠ” íŠ¹ì§•ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì„¹ì…˜ì—ì„œëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í°ì´ ì•„ë‹ˆë¼ ì§€ì •ëœ ê¸¸ì´ë¡œ ì—¬ëŸ¬ í† í°ì„ ìƒì„±í•˜ëŠ” ë° *generate()*ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ ì¤ë‹ˆë‹¤.

### í…ìŠ¤íŠ¸ ìƒì„±(Text Generation)

í…ìŠ¤íŠ¸ ìƒì„±(ê°œë°©í˜• í…ìŠ¤íŠ¸ ìƒì„±ì´ë¼ê³ ë„ í•¨)ì˜ ëª©í‘œëŠ” ì£¼ì–´ì§„ Contextì™€ ì¼ê´€ë˜ê²Œ ì´ì–´ì§€ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒ ì˜ˆëŠ” íŒŒì´í”„ë¼ì¸ì—ì„œ GPT-2ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ì€ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  ë•Œ ê° Configì—ì„œ ì„¤ì •í•œ ëŒ€ë¡œ Top-K ìƒ˜í”Œë§ì„ ì ìš©í•©ë‹ˆë‹¤(ì˜ˆì‹œ : [gpt-2 config](https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json) ì°¸ì¡°).

```python
from transformers import pipeline

text_generator = pipeline("text-generation")
print(text_generator("As far as I am concerned, I will", max_length=50, do_sample=False))

"""
[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a
"free market." I think that the idea of a free market is a bit of a stretch. I think that the idea'}]
"""
```

ì—¬ê¸°ì„œ ëª¨ë¸ì€ "*As far as I am concerned, I will*"ë¼ëŠ” Contextì—ì„œ ì´ ìµœëŒ€ ê¸¸ì´ 50ê°œì˜ í† í°ì„ ê°€ì§„ ì„ì˜ì˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë°±ê·¸ë¼ìš´ë“œì—ì„œ íŒŒì´í”„ë¼ì¸ ê°ì²´ëŠ”  *generate()* ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. *max_length* ë° *do_sample* ì¸ìˆ˜ì™€ ê°™ì´ ì´ ë©”ì„œë“œì˜ ê¸°ë³¸ ì¸ìˆ˜ëŠ” íŒŒì´í”„ë¼ì¸ì—ì„œ ì¬ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ *XLNet* ë° í•´ë‹¹ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì œì´ë©°, *generate()* ë©”ì„œë“œë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

```python
# Pytorch

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("xlnet-base-cased")
tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")

# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology
PADDING_TEXT = """In 1991, the remains of Russian Tsar Nicholas II and his family
(except for Alexei and Maria) are discovered.
The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the
remainder of the story. 1883 Western Siberia,
a young Grigori Rasputin is asked by his father and a group of men to perform magic.
Rasputin has a vision and denounces one of the men as a horse thief. Although his
father initially slaps him for making such an accusation, Rasputin watches as the
man is chased outside and beaten. Twenty years later, Rasputin sees a vision of
the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,
with people, even a bishop, begging for his blessing. <eod> </s> <eos>"""

prompt = "Today the weather is really nice and I am planning on "
inputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors="pt")["input_ids"]

prompt_length = len(tokenizer.decode(inputs[0]))
outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)
generated = prompt + tokenizer.decode(outputs[0])[prompt_length+1:]

print(generated)

"""
Today the weather is really nice and I am planning ...
"""
```

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("xlnet-base-cased")
tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")

# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology
PADDING_TEXT = """In 1991, the remains of Russian Tsar Nicholas II and his family
(except for Alexei and Maria) are discovered.
The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the
remainder of the story. 1883 Western Siberia,
a young Grigori Rasputin is asked by his father and a group of men to perform magic.
Rasputin has a vision and denounces one of the men as a horse thief. Although his
father initially slaps him for making such an accusation, Rasputin watches as the
man is chased outside and beaten. Twenty years later, Rasputin sees a vision of
the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,
with people, even a bishop, begging for his blessing. <eod> </s> <eos>"""

prompt = "Today the weather is really nice and I am planning on "
inputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors="pt")["input_ids"]

prompt_length = len(tokenizer.decode(inputs[0]))
outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)
generated = prompt + tokenizer.decode(outputs[0])[prompt_length+1:]

print(generated)

"""
Today the weather is really nice and I am planning ...
"""
```

í…ìŠ¤íŠ¸ ìƒì„±ì€ í˜„ì¬ PyTorchì˜ *GPT-2, OpenAi-GPT, CTRL, XLNet, Transpo-XL* ë° Reformerì™€ Tensorflowì˜ ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì—ì„œë„ ê°€ëŠ¥í•©ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, *XLNet* ë° *Transpo-XL*ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ë ¤ë©´ íŒ¨ë”©ì´ í•„ìš”í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. *GPT-2*ëŠ” ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§ ëª©ì ìœ¼ë¡œ ìˆ˜ë°±ë§Œ ê°œì˜ ì›¹ í˜ì´ì§€ë¥¼ í†µí•´ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ ê°œë°©í˜• í…ìŠ¤íŠ¸ ìƒì„±ì— ì í•©í•©ë‹ˆë‹¤.

í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•´ ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµì„ ì ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [í…ìŠ¤íŠ¸ ìƒì„± ë¸”ë¡œê·¸ ê²Œì‹œë¬¼](https://huggingface.co/blog/how-to-generate)ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

### ê°œì²´ëª… ì¸ì‹(Named Entity Recognition)

ê°œì²´ëª… ì¸ì‹(NER)ì€ ê°œì¸, ê¸°ê´€ ë˜ëŠ” ì¥ì†Œì˜ ì´ë¦„ ë“±ìœ¼ë¡œ ì‹ë³„ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ì— ë”°ë¼ í† í°ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ê°œì²´ëª… ì¸ì‹ ë°ì´í„°ì…‹ì˜ ì˜ˆë¡œëŠ” CoNLL-2003 ë°ì´í„°ì…‹ì´ ìˆìŠµë‹ˆë‹¤. NER ì‘ì—…ì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ëŠ” ê²½ìš° [run_ner.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py) ìŠ¤í¬ë¦½íŠ¸ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ê°œì²´ëª… ì¸ì‹ìœ¼ë¡œ í† í°ì„ 9ê°œ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ì— ì†í•˜ë„ë¡ ì˜ˆì¸¡í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤(BIO í‘œí˜„).

> **O**, ê°œì²´ëª…ì´ ì•„ë‹Œ ë¶€ë¶„
**B-MIS**, ê¸°íƒ€ ì—”í‹°í‹°ê°€ ì‹œì‘ë˜ëŠ” ë¶€ë¶„
**I-MIS**, ê¸°íƒ€ ì—”í‹°í‹° 
**B-PER**, ì‚¬ëŒì˜ ì´ë¦„ì´ ì‹œì‘ë˜ëŠ” ë¶€ë¶„
**I-PER**, ì‚¬ëŒì˜ ì´ë¦„
**B-ORG**, ê¸°ê´€ëª…ì´ ì‹œì‘ë˜ëŠ” ë¶€ë¶„
**I-ORG**, ê¸°ê´€ëª…
**B-LOC**, ì¥ì†Œëª…ì´ ì‹œì‘ë˜ëŠ” ë¶€ë¶„
**I-LOC**, ì¥ì†Œëª…
> 

CoNLL-2003ì˜ íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, dbmdzì˜ @stefan-itì— ì˜í•´ íŒŒì¸íŠœë‹ ë˜ì—ˆìŠµë‹ˆë‹¤.

```python
from transformers import pipeline

ner_pipe = pipeline("ner")

sequence = """Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO,
therefore very close to the Manhattan Bridge which is visible from the window."""
```

ì´ë ‡ê²Œ í•˜ë©´ ìœ„ì—ì„œ ì •ì˜í•œ 9ê°œ í´ë˜ìŠ¤ì˜ ì—”í‹°í‹° ì¤‘ í•˜ë‚˜ë¡œ ì‹ë³„ëœ ëª¨ë“  ë‹¨ì–´ ëª©ë¡ì´ ì¶œë ¥ë©ë‹ˆë‹¤. ì˜ˆìƒë˜ëŠ” ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
for entity in ner_pipe(sequence):
    print(entity)
"""
{'entity': 'I-ORG', 'score': 0.9996, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9910, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9982, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-ORG', 'score': 0.9995, 'index': 4, 'word': 'Inc', 'start': 13, 'end': 16}
{'entity': 'I-LOC', 'score': 0.9994, 'index': 11, 'word': 'New', 'start': 40, 'end': 43}
{'entity': 'I-LOC', 'score': 0.9993, 'index': 12, 'word': 'York', 'start': 44, 'end': 48}
{'entity': 'I-LOC', 'score': 0.9994, 'index': 13, 'word': 'City', 'start': 49, 'end': 53}
{'entity': 'I-LOC', 'score': 0.9863, 'index': 19, 'word': 'D', 'start': 79, 'end': 80}
{'entity': 'I-LOC', 'score': 0.9514, 'index': 20, 'word': '##UM', 'start': 80, 'end': 82}
{'entity': 'I-LOC', 'score': 0.9337, 'index': 21, 'word': '##BO', 'start': 82, 'end': 84}
{'entity': 'I-LOC', 'score': 0.9762, 'index': 28, 'word': 'Manhattan', 'start': 114, 'end': 123}
{'entity': 'I-LOC', 'score': 0.9915, 'index': 29, 'word': 'Bridge', 'start': 124, 'end': 130}
"""
```

ì–´ë–»ê²Œ "Huggingface" ì‹œí€€ìŠ¤ì˜ í† í°ì´ ê¸°ê´€ëª…ìœ¼ë¡œ ì‹ë³„ë˜ê³  "New York City", "DUMBO" ë° "Manhattan Bridge"ê°€ ì¥ì†Œëª…ìœ¼ë¡œ ì‹ë³„ë˜ëŠ”ì§€ì— ì£¼ì˜í•´ì„œ ë³´ì‹­ì‹œì˜¤.

ë‹¤ìŒì€ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œì²´ëª… ì¸ì‹ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. ì²´í¬í¬ì¸íŠ¸ì—ì„œ í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
2. ê° ì‹œí€€ìŠ¤ì˜ ì—”í‹°í‹°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ "Hugging Face"ë¥¼ ê¸°ê´€ëª…ìœ¼ë¡œ, "New York City"ë¥¼ ì¥ì†Œëª…ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. ë‹¨ì–´ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì˜ˆì¸¡ì— ë§¤í•‘í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¨¼ì € ì‹œí€€ìŠ¤ë¥¼ ì™„ì „íˆ ì¸ì½”ë”©í•˜ê³  ë””ì½”ë”©í•˜ì—¬ íŠ¹ë³„í•œ í† í°ì´ í¬í•¨ëœ ë¬¸ìì—´ì„ ë‚¨ê²¨ë‘ë„ë¡ í•©ë‹ˆë‹¤.
4. í•´ë‹¹ ì‹œí€€ìŠ¤ë¥¼ IDë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤(íŠ¹ìˆ˜ í† í°ì´ ìë™ìœ¼ë¡œ ì¶”ê°€ë¨).
5. ì…ë ¥ í† í°ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³ , ì²« ë²ˆì§¸ ì¶œë ¥ì„ ê°€ì ¸ì™€ì„œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ê° í† í°ì— ëŒ€í•´ ë§¤ì¹­ ê°€ëŠ¥í•œ 9ê°œ í´ë˜ìŠ¤ì™€ ëŒ€ì¡°í•©ë‹ˆë‹¤. ê° í† í°ì— ëŒ€í•´ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ í´ë˜ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•´ argmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
6. ê°ê°ì˜ í† í°ì„ ì˜ˆì¸¡ ê²°ê³¼ì™€ ë¬¶ì–´ í”„ë¦°íŠ¸í•©ë‹ˆë‹¤.

```python
# Pytorch
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

model = AutoModelForTokenClassification.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, " \
           "therefore very close to the Manhattan Bridge."

inputs = tokenizer(sequence, return_tensors="pt")
tokens = inputs.tokens()

outputs = model(**inputs).logits
predictions = torch.argmax(outputs, dim=2)
```

```python
# Tensorflow
from transformers import TFAutoModelForTokenClassification, AutoTokenizer
import tensorflow as tf

model = TFAutoModelForTokenClassification.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, " \
           "therefore very close to the Manhattan Bridge."

inputs = tokenizer(sequence, return_tensors="tf")
tokens = inputs.tokens()

outputs = model(**inputs)[0]
predictions = tf.argmax(outputs, axis=2)
```

í•´ë‹¹ ì˜ˆì¸¡ ê²°ê³¼ë¡œ ë§¤í•‘ëœ ê° í† í° ëª©ë¡ì„ ì¶œë ¥í•©ë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ê³¼ ë‹¬ë¦¬ ëª¨ë“  í† í°ì— ì˜ˆì¸¡ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ ë˜ëŠ”ë°, ì—”í‹°í‹°ê°€ ì—†ëŠ” í† í°ì¸ í´ë˜ìŠ¤ 0ì˜ ê²½ìš°ë¥¼ ì œê±°í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ìœ„ì˜ ì˜ˆì‹œì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì •ìˆ˜ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì •ìˆ˜ í˜•íƒœì˜ í´ë˜ìŠ¤ ë²ˆí˜¸ë¥¼ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ model.config.id2label ì†ì„±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
for token, prediction in zip(tokens, predictions[0].numpy()):
    print((token, model.config.id2label[prediction]))

"""
('[CLS]', 'O')
('Hu', 'I-ORG')
('##gging', 'I-ORG')
('Face', 'I-ORG')
('Inc', 'I-ORG')
('.', 'O')
('is', 'O')
('a', 'O')
('company', 'O')
('based', 'O')
('in', 'O')
('New', 'I-LOC')
('York', 'I-LOC')
('City', 'I-LOC')
('.', 'O')
('Its', 'O')
('headquarters', 'O')
('are', 'O')
('in', 'O')
('D', 'I-LOC')
('##UM', 'I-LOC')
('##BO', 'I-LOC')
(',', 'O')
('therefore', 'O')
('very', 'O')
('close', 'O')
('to', 'O')
('the', 'O')
('Manhattan', 'I-LOC')
('Bridge', 'I-LOC')
('.', 'O')
('[SEP]', 'O')
"""
```
