## Quick Tour : Under the hood

[ğŸ”— Huggingface Transformers Docs >> Under the hood : pretrained models](https://huggingface.co/transformers/quicktour.html#under-the-hood-pretrained-models)

ì´ì œ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•  ë•Œ ê·¸ ì•ˆì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ë³´ë©´, ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” *from_pretrained* ë©”ì„œë“œë¥¼ í†µí•´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.

```python
# Pytorch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

```python
# Tensorflow
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

### í† í¬ë‚˜ì´ì € ì‚¬ìš©í•˜ê¸°

í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ì˜ ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤. ë¨¼ì €, ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ í† í°(token) (ë˜ëŠ” ë‹¨ì–´ì˜ ì¼ë¶€, êµ¬ë‘ì  ê¸°í˜¸ ë“±)ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê·œì¹™ë“¤ì´ ìˆìœ¼ë¯€ë¡œ([í† í¬ë‚˜ì´ì € ìš”ì•½](https://huggingface.co/transformers/tokenizer_summary.html)ì—ì„œ ë” ìì„¸íˆ ì•Œì•„ë³¼ ìˆ˜ ìˆìŒ), ëª¨ë¸ëª…ì„ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•´ì•¼ë§Œ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ê³¼ ë™ì¼í•œ ê·œì¹™ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‘ë²ˆì§¸ ë‹¨ê³„ëŠ”, í† í°(token)ì„ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ í…ì„œ(tensor)ë¥¼ êµ¬ì¶•í•˜ê³  ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, í† í¬ë‚˜ì´ì €ì—ëŠ” *from_pretrained* ë©”ì„œë“œë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•  ë•Œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” *vocab*ì´ë¼ëŠ” ê²ƒì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ì‚¬ì „í•™ìŠµ ë˜ì—ˆì„ ë•Œì™€ ë™ì¼í•œ vocabì„ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ì´ ê³¼ì •ë“¤ì„ ì ìš©í•˜ë ¤ë©´ í† í¬ë‚˜ì´ì €ì— ì•„ë˜ì™€ ê°™ì´ í…ìŠ¤íŠ¸ë¥¼ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.

```python
inputs = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
```

ì´ë ‡ê²Œ í•˜ë©´, ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ë¬¸ìì—´ì´ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ ë¦¬ìŠ¤íŠ¸ëŠ” [í† í° ID](https://huggingface.co/transformers/glossary.html#input-ids)(ids of the tokens)ë¥¼ í¬í•¨í•˜ê³  ìˆê³ , ëª¨ë¸ì— í•„ìš”í•œ ì¶”ê°€ ì¸ìˆ˜ ë˜í•œ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ëª¨ë¸ì´ ì‹œí€€ìŠ¤ë¥¼ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” [ì–´í…ì…˜ ë§ˆìŠ¤í¬](https://huggingface.co/transformers/glossary.html#attention-mask)(attention mask)ë„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

```python
print(inputs)

"""
{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
"""
```

í† í¬ë‚˜ì´ì €ì— ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°°ì¹˜(batch)ë¡œ ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” ê²ƒì´ ëª©í‘œë¼ë©´, ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©í•˜ê³  ëª¨ë¸ì´ í—ˆìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ë¡œ ì˜ë¼ í…ì„œë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì— ì´ëŸ¬í•œ ì‚¬í•­ë“¤ì„ ëª¨ë‘ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

```python
# Pytorch
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt"
)
```

```python
# Tensorflow
tf_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf"
)
```

ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ëŠ” ìœ„ì¹˜ì—(ì´ ê°™ì€ ê²½ìš°ì—” ì˜¤ë¥¸ìª½) í”„ë¦¬íŠ¸ë ˆì´ë‹ëœ íŒ¨ë”© í† í°ì„ ì´ìš©í•˜ì—¬ íŒ¨ë”©ì´ ìë™ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤. ì–´í…ì…˜ ë§ˆìŠ¤í¬ë„ íŒ¨ë”©ì„ ê³ ë ¤í•˜ì—¬ ì¡°ì •ë©ë‹ˆë‹¤.

```python
# Pytorch
for key, value in pt_batch.items():
    print(f"{key}: {value.numpy().tolist()}"

"""
input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]
attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]
"""
```

```python
# Tensorflow
for key, value in tf_batch.items():
    print(f"{key}: {value.numpy().tolist()}")

"""
input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]
attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]
"""
```

í† í¬ë‚˜ì´ì €ì— ëŒ€í•´ [ì´ê³³](https://huggingface.co/transformers/preprocessing.html)ì—ì„œ ë” ìì„¸íˆ ì•Œì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

ì¸í’‹ ë°ì´í„°ê°€ í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ì „ì²˜ë¦¬ë˜ë©´, ëª¨ë¸ë¡œ ì§ì ‘ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼, ëª¨ë¸ì— í•„ìš”í•œ ëª¨ë“  ê´€ë ¨ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤. ë§Œì•½ í…ì„œí”Œë¡œìš° ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤ë©´ ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ë¥¼ ì§ì ‘ í…ì„œë¡œ ì „ë‹¬í•  ìˆ˜ ìˆê³ , íŒŒì´í† ì¹˜ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤ë©´ '**'ì„ ë”í•´ì„œ ë”•ì…”ë„ˆë¦¬ë¥¼ í’€ì–´ ì¤˜ì•¼ í•©ë‹ˆë‹¤.

```python
# Pytorch
pt_outputs = pt_model(**pt_batch)
```

```python
# Tensorflow
tf_outputs = tf_model(tf_batch)
```

í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ëª¨ë“  ì•„ì›ƒí’‹ì€ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ëª¨ë¸ì˜ ìµœì¢… í™œì„±í™” ìƒíƒœê°€ í¬í•¨ëœ ê°œì²´ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê°œì²´ëŠ” ì—¬ê¸°ì— ë” ìì„¸íˆ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¶œë ¥ê°’ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# Pytorch
print(pt_outputs)

"""
SequenceClassifierOutput(loss=None, logits=tensor([[-4.0833,  4.3364],
       [ 0.0818, -0.0418]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)
"""
```

```python
# Tensorflow
print(tf_outputs)
"""
TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[-4.0833 ,  4.3364  ],
       [ 0.0818, -0.0418]], dtype=float32)>, hidden_states=None, attentions=None)
"""
```

ì¶œë ¥ëœ ê°’ì— ìˆëŠ” *logits* í•­ëª©ì— ì£¼ëª©í•˜ì‹­ì‹œì˜¤. ì´ í•­ëª©ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ìµœì¢… í™œì„±í™” ìƒíƒœì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì£¼ì˜
ëª¨ë“  í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸(íŒŒì´í† ì¹˜ ë˜ëŠ” í…ì„œí”Œë¡œìš°)ì€ ë§ˆì§€ë§‰ í™œì„±í™” í•¨ìˆ˜ê°€ ì¢…ì¢… ì†ì‹¤(loss)ê³¼ ë”í•´ì§€ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ í™œì„±í™” í•¨ìˆ˜(ì†Œí”„íŠ¸ë§¥ìŠ¤ ê°™ì€)ë¥¼ ì ìš©í•˜ê¸° ì´ì „ì˜ ëª¨ë¸ í™œì„±í™” ìƒíƒœë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤. 

ì˜ˆì¸¡ì„ ìœ„í•´ ì†Œí”„íŠ¸ë§¥ìŠ¤ í™œì„±í™”ë¥¼ ì ìš©í•´ ë´…ì‹œë‹¤.

```python
# Pytorch
from torch import nn
pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
```

```python
# Tensorflow
import tensorflow as tf
tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
```

ì´ì „ ê³¼ì •ì—ì„œ ì–»ì–´ì§„ ìˆ«ìë“¤ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# Pytorch
print(pt_predictions)
"""
tensor([[2.2043e-04, 9.9978e-01],
        [5.3086e-01, 4.6914e-01]], grad_fn=<SoftmaxBackward>)
"""
```

```python
# Tensorflow
print(tf_predictions)
"""
tf.Tensor(
[[2.2043e-04 9.9978e-01]
 [5.3086e-01 4.6914e-01]], shape=(2, 2), dtype=float32)
"""
```

ëª¨ë¸ì— ì¸í’‹ ë°ì´í„° ì™¸ì— ë¼ë²¨ì„ ë„£ëŠ” ê²½ìš°ì—ëŠ”, ëª¨ë¸ ì¶œë ¥ ê°œì²´ì— ë‹¤ìŒê³¼ ê°™ì€ ì†ì‹¤(loss) ì†ì„±ë„ í¬í•¨ë©ë‹ˆë‹¤.

```python
# Pytorch
import torch
pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))
print(pt_outputs)
"""
SequenceClassifierOutput(loss=tensor(0.3167, grad_fn=<NllLossBackward>), logits=tensor([[-4.0833,  4.3364],
        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)
"""
```

```python
# Tensorflow
import tensorflow as tf
tf_outputs = tf_model(tf_batch, labels = tf.constant([1, 0]))
print(tf_outputs)
"""
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.2051e-04, 6.3326e-01], dtype=float32)>, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[-4.0833 ,  4.3364  ],
       [ 0.0818, -0.0418]], dtype=float32)>, hidden_states=None, attentions=None)
"""
```

ëª¨ë¸ì€ í‘œì¤€ [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ì´ë‚˜ [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ë¡œ íŠ¸ë ˆì´ë‹ ë£¨í”„ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” Trainer(í…ì„œí”Œë¡œìš°ì—ì„œëŠ” TFTrainer) í´ë˜ìŠ¤ë¥¼ ì œê³µí•˜ì—¬ ì—¬ëŸ¬ë¶„ì´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ ë•ìŠµë‹ˆë‹¤(ë¶„ì‚° íŠ¸ë ˆì´ë‹, í˜¼í•© ì •ë°€ë„ ë“±ê³¼ ê°™ì€ ê³¼ì •ì—ì„œëŠ” ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤). ìì„¸í•œ ë‚´ìš©ì€ [íŠ¸ë ˆì´ë‹ íŠœí† ë¦¬ì–¼](https://huggingface.co/transformers/training.html)ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

ì£¼ì˜
Pytorch ëª¨ë¸ ì¶œë ¥ì€ IDEì˜ ì†ì„±ì— ëŒ€í•œ ìë™ ì™„ì„±ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” íŠ¹ìˆ˜ ë°ì´í„° í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ë˜í•œ íŠœí”Œ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ ì‘ë™í•©ë‹ˆë‹¤(ì •ìˆ˜, ìŠ¬ë¼ì´ìŠ¤ ë˜ëŠ” ë¬¸ìì—´ë¡œ ì¸ë±ì‹±í•  ìˆ˜ ìˆìŒ). ì´ ê²½ìš° ì„¤ì •ë˜ì§€ ì•Šì€ ì†ì„±(None ê°’ì„ ê°€ì§€ê³  ìˆëŠ”)ì€ ë¬´ì‹œë©ë‹ˆë‹¤.

ëª¨ë¸ì˜ íŒŒì¸íŠœë‹ì´ ëë‚˜ë©´, ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ì™€ í•¨ê»˜ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)
```

ê·¸ëŸ° ë‹¤ìŒ ëª¨ë¸ ì´ë¦„ ëŒ€ì‹  ë””ë ‰í† ë¦¬ ì´ë¦„ì„ ì „ë‹¬í•˜ì—¬ from_pretrained() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì˜ ë©‹ì§„ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” íŒŒì´í† ì¹˜ì™€ í…ì„œí”Œë¡œìš° ê°„ì— ì‰½ê²Œ ì „í™˜í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ì „ê³¼ ê°™ì´ ì €ì¥ëœ ëª¨ë¸ì€ íŒŒì´í† ì¹˜ ë˜ëŠ” í…ì„œí”Œë¡œìš°ì—ì„œ ë‹¤ì‹œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €ì¥ëœ íŒŒì´í† ì¹˜ ëª¨ë¸ì„ í…ì„œí”Œë¡œìš° ëª¨ë¸ì— ë¡œë“œí•˜ëŠ” ê²½ìš° from_pretrained()ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
# Pytorch -> Tensorflow
from transformers import TFAutoModel
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = TFAutoModel.from_pretrained(save_directory, from_pt=True)
```

ì €ì¥ëœ í…ì„œí”Œë¡œìš° ëª¨ë¸ì„ íŒŒì´í† ì¹˜ ëª¨ë¸ì— ë¡œë“œí•˜ëŠ” ê²½ìš° ë‹¤ìŒ ì½”ë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

```python
# Tensorflow -> Pytorch
from transformers import AutoModel
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = AutoModel.from_pretrained(save_directory, from_tf=True)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, ëª¨ë¸ì˜ ëª¨ë“  ì€ë‹‰ ìƒíƒœ(hidden state)ì™€ ëª¨ë“  ì–´í…ì…˜ ê°€ì¤‘ì¹˜(attention weight)ë¥¼ ë¦¬í„´í•˜ë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# Pytorch
pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states  = pt_outputs.hidden_states
all_attentions = pt_outputs.attentions
```

```python
# Tensorflow
tf_outputs = tf_model(tf_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states =  tf_outputs.hidden_states
all_attentions = tf_outputs.attentions
```

### ì½”ë“œì— ì—‘ì„¸ìŠ¤í•˜ê¸°

*AutoModel* ë° *AutoTokenizer* í´ë˜ìŠ¤ëŠ” ì‚¬ì „ êµìœ¡ëœ ëª¨ë¸ë¡œ ìë™ìœ¼ë¡œ ì´ë™í•  ìˆ˜ ìˆëŠ” ë°”ë¡œê°€ê¸°ì¼ ë¿ì…ë‹ˆë‹¤. ì´ë©´ì—ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì•„í‚¤í…ì²˜ì™€ í´ë˜ìŠ¤ì˜ ì¡°í•©ë‹¹ í•˜ë‚˜ì˜ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ í•„ìš”ì— ë”°ë¼ ì½”ë“œë¥¼ ì‰½ê²Œ ì•¡ì„¸ìŠ¤í•˜ê³  ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì „ ì˜ˆì‹œì—ì„œ, ì´ ëª¨ë¸ì€ '*distilbert-base-cased-un-finetuned-sst-2-english*'ë¼ê³  ë¶ˆë ¸ëŠ”ë°, ì´ëŠ” *[DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)* êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. *AutoModelForSequenceClassification*(ë˜ëŠ” í…ì„œí”Œë¡œìš°ì—ì„œëŠ” *TFAutoModelForSequenceClassification*)ì´ ì‚¬ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ ìë™ìœ¼ë¡œ ìƒì„±ëœ ëª¨ë¸ì€ *DistilBertForSequenceClassification*ì´ ë©ë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì˜ ì„¤ëª…ì„œì—ì„œ í•´ë‹¹ ëª¨ë¸ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì„¸ë¶€ ì •ë³´ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ ì§ì ‘ ì¸ìŠ¤í„´ìŠ¤í™”í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
# Pytorch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = DistilBertForSequenceClassification.from_pretrained(model_name)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```

```python
# Tensorflow
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFDistilBertForSequenceClassification.from_pretrained(model_name)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```

### ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•˜ê¸°

ëª¨ë¸ ìì²´ì˜ ë¹Œë“œ ë°©ë²•ì„ ë³€ê²½í•˜ë ¤ë©´ ì‚¬ìš©ì ì •ì˜ êµ¬ì„± í´ë˜ìŠ¤ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ì•„í‚¤í…ì²˜ì—ëŠ” ê³ ìœ í•œ ê´€ë ¨ êµ¬ì„±(Configuration)ì´ ì œê³µë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [*DistilBertConfig*](https://huggingface.co/transformers/model_doc/distilbert.html#transformers.DistilBertConfig)ë¥¼ ì‚¬ìš©í•˜ë©´ *DistilBERT*ì— ëŒ€í•œ ì€ë‹‰ ì°¨ì›(hidden dimension), ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨(dropout rate) ë“±ì˜ ë§¤ê°œë³€ìˆ˜(parameter)ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì€ë‹‰ ì°¨ì›ì˜ í¬ê¸°ë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ì¤‘ìš”í•œ ìˆ˜ì • ì‘ì—…ì„ í•˜ë©´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë” ì´ìƒ ì‚¬ìš©í•  ìˆ˜ ì—†ê³  ì²˜ìŒë¶€í„° í•™ìŠµì‹œì¼œì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ Configì—ì„œ ì§ì ‘ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.

ì•„ë˜ì—ì„œëŠ” from_pretrained() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ì— ì‚¬ì „ ì •ì˜ëœ ì–´íœ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í† í¬ë‚˜ì´ì €ì™€ ë‹¬ë¦¬ ìš°ë¦¬ëŠ” ì²˜ìŒë¶€í„° ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³ ì í•©ë‹ˆë‹¤. ë”°ë¼ì„œ from_pretrained() ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  Configì—ì„œ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.

```python
# Pytorch
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification(config)
```

```python
# Tensorflow
from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification
config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = TFDistilBertForSequenceClassification(config)
```

ëª¨ë¸ í—¤ë“œë§Œ ë³€ê²½í•˜ëŠ” ê²½ìš°(ë¼ë²¨ ìˆ˜ì™€ ê°™ì€)ì—ë„ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 10ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë¼ë²¨ì— ëŒ€í•œ ë¶„ë¥˜ê¸°(Classifier)ë¥¼ ì •ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë¼ë²¨ ìˆ˜ë¥¼ ë³€ê²½í•˜ê¸° ìœ„í•´ ëª¨ë“  ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆ Configë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ì‹ ì— Configê°€ from_pretrained() ë©”ì„œë“œì— ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ë©´ ê¸°ë³¸ Configê°€ ì ì ˆíˆ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.

```python
# Pytorch
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```

```python
# Tensorflow
from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification
model_name = "distilbert-base-uncased"
model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```
