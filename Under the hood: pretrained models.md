## Quick Tour : Under the hood

[ğŸ”— Huggingface Transformers Docs >> Under the hood : pretrained models](https://huggingface.co/transformers/quicktour.html#under-the-hood-pretrained-models)

ì´ì œ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•  ë•Œ ê·¸ ì•ˆì—ì„œ ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ë³´ë©´, ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ëŠ” *from_pretrained* ë©”ì„œë“œë¥¼ í†µí•´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.

```python
# Pytorch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

```python
# Tensorflow
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

### í† í¬ë‚˜ì´ì € ì‚¬ìš©í•˜ê¸°

í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ì˜ ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤. ë¨¼ì €, ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ í† í°(token) (ë˜ëŠ” ë‹¨ì–´ì˜ ì¼ë¶€, êµ¬ë‘ì  ê¸°í˜¸ ë“±)ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê·œì¹™ë“¤ì´ ìˆìœ¼ë¯€ë¡œ([í† í¬ë‚˜ì´ì € ìš”ì•½](https://huggingface.co/transformers/tokenizer_summary.html)ì—ì„œ ë” ìì„¸íˆ ì•Œì•„ë³¼ ìˆ˜ ìˆìŒ), ëª¨ë¸ëª…ì„ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•´ì•¼ë§Œ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ê³¼ ë™ì¼í•œ ê·œì¹™ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‘ë²ˆì§¸ ë‹¨ê³„ëŠ”, í† í°(token)ì„ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ í…ì„œ(tensor)ë¥¼ êµ¬ì¶•í•˜ê³  ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, í† í¬ë‚˜ì´ì €ì—ëŠ” *from_pretrained* ë©”ì„œë“œë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•  ë•Œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” *vocab*ì´ë¼ëŠ” ê²ƒì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ì‚¬ì „í•™ìŠµ ë˜ì—ˆì„ ë•Œì™€ ë™ì¼í•œ vocabì„ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ì´ ê³¼ì •ë“¤ì„ ì ìš©í•˜ë ¤ë©´ í† í¬ë‚˜ì´ì €ì— ì•„ë˜ì™€ ê°™ì´ í…ìŠ¤íŠ¸ë¥¼ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.

```python
inputs = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
```

ì´ë ‡ê²Œ í•˜ë©´, ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ë¬¸ìì—´ì´ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ ë¦¬ìŠ¤íŠ¸ëŠ” [í† í° ID](https://huggingface.co/transformers/glossary.html#input-ids)(ids of the tokens)ë¥¼ í¬í•¨í•˜ê³  ìˆê³ , ëª¨ë¸ì— í•„ìš”í•œ ì¶”ê°€ ì¸ìˆ˜ ë˜í•œ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ëª¨ë¸ì´ ì‹œí€€ìŠ¤ë¥¼ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” [ì–´í…ì…˜ ë§ˆìŠ¤í¬](https://huggingface.co/transformers/glossary.html#attention-mask)(attention mask)ë„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

```python
print(inputs)

"""
{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
"""
```

í† í¬ë‚˜ì´ì €ì— ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°°ì¹˜(batch)ë¡œ ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” ê²ƒì´ ëª©í‘œë¼ë©´, ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©í•˜ê³  ëª¨ë¸ì´ í—ˆìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ë¡œ ì˜ë¼ í…ì„œë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì— ì´ëŸ¬í•œ ì‚¬í•­ë“¤ì„ ëª¨ë‘ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

```python
# Pytorch
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt"
)
```

```python
# Tensorflow
tf_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf"
)
```

ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ëŠ” ìœ„ì¹˜ì—(ì´ ê°™ì€ ê²½ìš°ì—” ì˜¤ë¥¸ìª½) í”„ë¦¬íŠ¸ë ˆì´ë‹ëœ íŒ¨ë”© í† í°ì„ ì´ìš©í•˜ì—¬ íŒ¨ë”©ì´ ìë™ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤. ì–´í…ì…˜ ë§ˆìŠ¤í¬ë„ íŒ¨ë”©ì„ ê³ ë ¤í•˜ì—¬ ì¡°ì •ë©ë‹ˆë‹¤.

```python
# Pytorch
for key, value in pt_batch.items():
    print(f"{key}: {value.numpy().tolist()}"

"""
input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]
attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]
"""
```

```python
# Tensorflow
for key, value in tf_batch.items():
    print(f"{key}: {value.numpy().tolist()}")

"""
input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]
attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]
"""
```

í† í¬ë‚˜ì´ì €ì— ëŒ€í•´ [ì´ê³³](https://huggingface.co/transformers/preprocessing.html)ì—ì„œ ë” ìì„¸íˆ ì•Œì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

